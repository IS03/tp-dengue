# üîπ Pasos de Transformaci√≥n (T en ETL)

## üü¶ 1. Preprocesamiento
En el preprocesamiento organizamos y limpiamos la estructura de los datos (nombres, tipos, fechas, duplicados, consistencia), lo que sirve para dejar el dataset en un formato coherente y confiable.

### 1.1 Normalizaci√≥n de estructura
- Renombrar columnas (min√∫sculas, sin espacios, con `_`).  
- Ordenar columnas seg√∫n l√≥gica (ej. primero fechas, luego mediciones).  
- Eliminar columnas redundantes o irrelevantes.  

---

### 1.2 Tipificaci√≥n de datos
- Convertir columnas de fechas con `pd.to_datetime`.  
- Convertir num√©ricas a `float` o `int` seg√∫n corresponda.  
- Convertir categ√≥ricas a `category`.  
- Convertir booleanos a `True/False` o `0/1`.  

---

### 1.3 Consistencia de datos
- Rango v√°lido (ej: humedad 0‚Äì100%).  
- Coherencia entre columnas (ej: `temperatura_min <= temperatura_max`).  
- Unidades homog√©neas (ej: ¬∞C en todas, no mezclar con ¬∞F).  
- Homogeneizar formatos de texto (may√∫sculas/min√∫sculas).  
- Unificar categor√≠as con nombres distintos pero mismo significado.  

---


## üü© 2. Procesamiento
En el procesamiento imputamos nulos, tratamos outliers, escalamos variables y generamos nuevas caracter√≠sticas, lo que sirve para preparar los datos de cada estaci√≥n de forma correcta y lista para el modelado.

### 2.1 Duplicados
- Detecci√≥n de filas duplicadas (`duplicated()`).  
- Eliminar duplicados exactos o mantener el primero.  
- Definir criterios de duplicado (ej: misma fecha y estaci√≥n).  

---

### 2.2 Valores faltantes (nulos)
- Calcular porcentaje de nulos por columna.  
- Decidir estrategia:
  - Eliminar columna (si >80% nulos).  
  - Eliminar filas (si pocos casos y no afectan representatividad).  
  - Imputar:
    - Media, mediana, moda.  
    - Forward-fill / Backward-fill.  
    - Interpolaci√≥n (lineal, polin√≥mica, spline).  
    - Valores espec√≠ficos (`0`, `"desconocido"`).  

> ‚ö†Ô∏è **Importante:** la imputaci√≥n debe realizarse por separado en cada tabla de estaci√≥n para evitar rellenar valores con datos de otra estaci√≥n por error.  

---

### 2.3 Valores at√≠picos (outliers)
- Identificarlos con:
  - Boxplot / IQR.  
  - Z-score / desviaci√≥n est√°ndar.  
  - Percentiles extremos (<1% o >99%).  
- Estrategias:
  - Mantener si son reales.  
  - Reemplazar con mediana o l√≠mite permitido.  
  - Eliminar si son errores evidentes.  

---

### 2.4 Normalizaci√≥n y escalado
- Escalado **Min-Max** ‚Üí valores entre 0 y 1.  
- **Log-transform** ‚Üí para distribuciones sesgadas (ej: lluvia, ingresos).  
- **Power transform** (Box-Cox / Yeo-Johnson) ‚Üí para reducir asimetr√≠a.  
- **Clipping** ‚Üí limitar valores a un rango razonable.  

---

### 2.5 Codificaci√≥n de variables categ√≥ricas
- **Label Encoding** (para ordinales).  
- **One-Hot Encoding** (para nominales).  
- **Binary Encoding** / **Target Encoding** (para muchas categor√≠as).  

---

### 2.6 Creaci√≥n de nuevas variables (feature engineering)
- Ratios entre columnas (ej: `temp_max/temp_min`).  
- Variables acumuladas o diferencias (`cumsum`, `diff`).  
- Interacciones entre variables (producto, suma, resta).  
- Variables categ√≥ricas agrupadas (ej: agrupar provincias en regiones).  

---

### 2.7 Balance de datos (para modelos predictivos)
- Detectar clases desbalanceadas en variables objetivo.  
- T√©cnicas:
  - Submuestreo / sobremuestreo.  
  - **SMOTE** (sint√©tico).  
  - Ponderaci√≥n de clases.  

---

### 2.8 Validaci√≥n final
- Revisi√≥n de estad√≠sticos descriptivos despu√©s de transformaciones.  
- Visualizaci√≥n r√°pida (histogramas, boxplots).  
- Comparaci√≥n con datos crudos para confirmar consistencia.  
